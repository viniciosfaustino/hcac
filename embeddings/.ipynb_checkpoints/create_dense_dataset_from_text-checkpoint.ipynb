{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('cbow_s50.txt')\n",
    "#this script should get a dataset which each row is a tuple, with the text and the label and transform it\n",
    "#in a tuple with the array of the embeddings and the label\n",
    "\n",
    "#first step should be load the previous dataset\n",
    "#then should be done some preprocessing to get the embeddings of the sentence\n",
    "#after getting all the embeddings of the sentences, the arrays should be reshaped with the maximum dimension \n",
    "\n",
    "embeddings = []\n",
    "dense_dateset = []\n",
    "max_dim = (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim = (0,0)\n",
    "#-----------------------------------------------------------\n",
    "#the preprocessing step also should split the words\n",
    "def preprocessing(sentence):\n",
    "    #translator = str.maketrans('', '', string.punctuation)\n",
    "    #for i,se in enumerate(sentences):\n",
    "    se = sentence\n",
    "    se = se.lower()\n",
    "    #se = se.translate(string.punctuation)\n",
    "    se = se.translate(None, string.punctuation)\n",
    "    se = se.replace(\".\",\"\")\n",
    "    se = \" \".join(se.split())\n",
    "    sentence = se.split()\n",
    "    #for j in range(len(sentences[i])):\n",
    "        #sentences[i][j] = sentences[i][j].decode(\"utf-8\")\n",
    "    #se = se.translate(translator)\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#this function should diferentiate a path to a dataset and a actual dataset array-like\n",
    "def load_dataset(dataset):\n",
    "    \n",
    "    sentences = dataset[0]\n",
    "    labels = dataset[1]\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "#this function should get the embedding of the sentences\n",
    "def get_embeddings(sentences, labels=None, max_dim=None):        \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        print(\"sentence #\",i)\n",
    "        sentence = preprocessing(sentence)\n",
    "        emb = []\n",
    "        for w in sentence:    \n",
    "            try:\n",
    "                emb.append(model.word_vec(str(w).decode(\"utf-8\")))\n",
    "            except:\n",
    "                emb.append(np.zeros(50))\n",
    "        emb = np.array(emb)\n",
    "        print(\"emb shape: \",emb.shape)\n",
    "        #row = [emb,label[i]]\n",
    "        embeddings.append(emb)\n",
    "        max_dim = max(max_dim, emb.shape)\n",
    "    print(len(embeddings))\n",
    "    return embeddings, max_dim\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#as long as the sentences have a different number of words and for computational purposes the shape of them must be equal\n",
    "#this function will set the default shape as the max of all the sentences, doing some zero padding to all the ones that doesn't fit the shape\n",
    "def do_padding(embeddings, max_dim, label):    \n",
    "    for i,em in enumerate(embeddings):\n",
    "        if em.shape != max_dim:\n",
    "            new_row = np.zeros(max_dim[0]*max_dim[1]).reshape(max_dim)\n",
    "            new_row[0:em.shape[0],0:em.shape[1]] += em\n",
    "            embeddings[i] = new_row\n",
    "        print(\"ih deu ruim \",i)        \n",
    "        dense_dataset.append([em,label[i]])\n",
    "        \n",
    "    return dense_dataset\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#this function should get the dataset from a file specified in the function calling or as an argument of the script\n",
    "def load_dataset_from_file(path):\n",
    "    if path != None:\n",
    "        input = path\n",
    "    else:\n",
    "        input = sys.argv[1]\n",
    "        \n",
    "    data = []\n",
    "    target = []\n",
    "    file = open(input, \"r\")\n",
    "    for line in file:\n",
    "        print line\n",
    "        data.append([str(line.split(\"\\t\")[0]).decode('utf8')])\n",
    "        target.append([int(line.split(\"\\t\")[1])])\n",
    "    file.close()\n",
    "    data = np.array(data)\n",
    "    target = np.array(target)\n",
    "    dataset = [data,target]\n",
    "    return dataset\n",
    "\n",
    "def from_text_to_embeddings():\n",
    "    input_path = sys.argv[1]\n",
    "    output_path = sys.argv[2]    \n",
    "    dataset = load_dataset_from_file(input_path)\n",
    "    setences,labels = load_dataset(dataset)\n",
    "    sentences = preprocessing(sentences)\n",
    "    embeddings, max_dim = get_embeddings(sentences)\n",
    "    dense_dateset = do_padding(embeddings, max_dim, label)\n",
    "    return dense_dataset\n",
    "\n",
    "def save_to_file(dense_dataset):\n",
    "    output = sys.argv[2]\n",
    "    np.savetxt(output, data)\n",
    "    \n",
    "def main():\n",
    "    dense_dateset = from_text_to_embeddings()\n",
    "    save_to_file(dense_dateset)\n",
    "    return\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sentence #', 0)\n",
      "('emb shape: ', (6, 50))\n",
      "('sentence #', 1)\n",
      "('emb shape: ', (10, 50))\n",
      "2\n",
      "(10, 50)\n",
      "('ih deu ruim ', 0)\n",
      "('ih deu ruim ', 1)\n",
      "(10, 50)\n",
      "O vice é um idiota # . , hue\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "dense_dateset = []\n",
    "max_dim = (0,0)\n",
    "s = [\"O vice é um idiota # . , hue\", \"Eu só quiria que tdo usso acabasse, mas não consigo . . . ~\"]\n",
    "e,m = get_embeddings(s, max_dim=max_dim)\n",
    "print(m)    \n",
    "label = [0 for i in range(2)]\n",
    "dataset = do_padding(e,m, label)\n",
    "save_to_file(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
